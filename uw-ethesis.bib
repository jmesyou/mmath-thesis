@book{java:vm-spec,
	title={The Java Virtual Machine Specification, Java SE 7 Edition: Java Virt Mach Spec Java\_3},
	author={Lindholm, Tim and Yellin, Frank and Bracha, Gilad and Buckley, Alex},
	year={2013},
	publisher={Addison-Wesley}
}


@inproceedings{truffle:partial-eval,
	author = {W\"{u}rthinger, Thomas and Wimmer, Christian and Humer, Christian and W\"{o}\ss{}, Andreas and Stadler, Lukas and Seaton, Chris and Duboscq, Gilles and Simon, Doug and Grimmer, Matthias},
	title = {Practical Partial Evaluation for High-Performance Dynamic Language Runtimes},
	year = {2017},
	isbn = {9781450349888},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3062341.3062381},
	doi = {10.1145/3062341.3062381},
	abstract = {Most high-performance dynamic language virtual machines duplicate language semantics in the interpreter, compiler, and runtime system. This violates the principle to not repeat yourself. In contrast, we define languages solely by writing an interpreter. The interpreter performs specializations, e.g., augments the interpreted program with type information and profiling information. Compiled code is derived automatically using partial evaluation while incorporating these specializations. This makes partial evaluation practical in the context of dynamic languages: It reduces the size of the compiled code while still compiling all parts of an operation that are relevant for a particular program. When a speculation fails, execution transfers back to the interpreter, the program re-specializes in the interpreter, and later partial evaluation again transforms the new state of the interpreter to compiled code. We evaluate our approach by comparing our implementations of JavaScript, Ruby, and R with best-in-class specialized production implementations. Our general-purpose compilation system is competitive with production systems even when they have been heavily optimized for the one language they support. For our set of benchmarks, our speedup relative to the V8 JavaScript VM is 0.83x, relative to JRuby is 3.8x, and relative to GNU R is 5x.},
	booktitle = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation},
	pages = {662–676},
	numpages = {15},
	keywords = {language implementation, optimization, partial evaluation, virtual machine, dynamic languages},
	location = {Barcelona, Spain},
	series = {PLDI 2017}
}


@phdthesis{truffle:thesis,
	address = {Linz, Austria},
	title = {Truffle {DSL}: {A} {DSL} for {Building} {Self}-{Optimizing} {AST} {Interpreters}},
	shorttitle = {Truffle {DSL}},
	abstract = {Self-optimizing AST interpreters dynamically adapt to the provided input for faster execution. This adaptation includes initial tests of the input, changes to AST nodes, and insertion of guards which ensure that assumptions still hold. Such specialization and speculation are essential for the performance of dynamic programming languages such as JavaScript, Ruby, and R. This thesis describes a declarative domain-speciﬁc language (DSL) that greatly simpliﬁes writing self-optimizing AST interpreters. The DSL supports specialization of operations based on types of the input arguments and other properties. The declared specializations are ﬂexible enough to express inline caches, a cornerstone of modern compiler optimizations.},
	language = {en},
	school = {Johannes Kepler University Linz},
	author = {Humer, Christian},
	year = {2016},
	note = {Publisher: Unpublished},
	file = {Humer - 2016 - Truffle DSL.pdf:/home/jyou/snap/zotero-snap/common/Zotero/storage/RI3VMB8S/Humer - 2016 - Truffle DSL A DSL for Building Self-Optimizing AS.pdf:application/pdf},
}


@article{mechanical-eval-of-exprs,
	title={The Mechanical Evaluation of Expressions},
	author={Peter J. Landin},
	journal={Comput. J.},
	year={1964},
	volume={6},
	pages={308-320}
}

@inproceedings{escape-analysis,
	author = {Kotzmann, Thomas and M\"{o}ssenb\"{o}ck, Hanspeter},
	title = {Escape Analysis in the Context of Dynamic Compilation and Deoptimization},
	year = {2005},
	isbn = {1595930477},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1064979.1064996},
	doi = {10.1145/1064979.1064996},
	abstract = {In object-oriented programming languages, an object is said to escape the method or thread in which it was created if it can also be accessed by other methods or threads. Knowing which objects do not escape allows a compiler to perform aggressive optimizations.This paper presents a new intraprocedural and interprocedural algorithm for escape analysis in the context of dynamic compilation where the compiler has to cope with dynamic class loading and deoptimization. It was implemented for Sun Microsystems' Java HotSpot™ client compiler and operates on an intermediate representation in SSA form. We introduce equi-escape sets for the efficient propagation of escape information between related objects. The analysis is used for scalar replacement of fields and synchronization removal, as well as for stack allocation of objects and fixed-sized arrays. The results of the interprocedural analysis support the compiler in inlining decisions and allow actual parameters to be allocated on the caller stack.Under certain circumstances, the Java HotSpot™ VM is forced to stop executing a method's machine code and transfer control to the interpreter. This is called deoptimization. Since the interpreter does not know about the scalar replacement and synchronization removal performed by the compiler, the deoptimization framework was extended to reallocate and relock objects on demand.},
	booktitle = {Proceedings of the 1st ACM/USENIX International Conference on Virtual Execution Environments},
	pages = {111–120},
	numpages = {10},
	keywords = {optimization, just-in-time compilation, stack allocation, deoptimization, escape analysis, synchronization removal, Java, scalar replacement},
	location = {Chicago, IL, USA},
	series = {VEE '05}
}

@inproceedings{scala:origins,
	title={Scalable component abstractions},
	author={Odersky, Martin and Zenger, Matthias},
	booktitle={Proceedings of the 20th annual ACM SIGPLAN conference on Object-oriented programming, systems, languages, and applications},
	pages={41--57},
	year={2005}
}


@misc{scala:lang-spec,
	title={The Scala language specification},
	author={Odersky, Martin and Altherr, Philippe and Cremet, Vincent and Emir, Burak and Micheloud, Stphane and Mihaylov, Nikolay and Schinz, Michel and Stenman, Erik and Zenger, Matthias},
	year={2004},
	publisher={Citeseer}
}

@article{scala:overview,
	title={An overview of the Scala programming language},
	author={Odersky, Martin and Altherr, Philippe and Cremet, Vincent and Emir, Burak and Maneth, Sebastian and Micheloud, St{\'e}phane and Mihaylov, Nikolay and Schinz, Michel and Stenman, Erik and Zenger, Matthias},
	year={2004}
}

@inproceedings{scala:collections-optimization,
	author = {Prokopec, Aleksandar and Leopoldseder, David and Duboscq, Gilles and W\"{u}rthinger, Thomas},
	title = {Making Collection Operations Optimal with Aggressive JIT Compilation},
	year = {2017},
	isbn = {9781450355292},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3136000.3136002},
	doi = {10.1145/3136000.3136002},
	abstract = {Functional collection combinators are a neat and widely accepted data processing abstraction. However, their generic nature results in high abstraction overheads -- Scala collections are known to be notoriously slow for typical tasks. We show that proper optimizations in a JIT compiler can widely eliminate overheads imposed by these abstractions. Using the open-source Graal JIT compiler, we achieve speedups of up to 20x on collection workloads compared to the standard HotSpot C2 compiler. Consequently, a sufficiently aggressive JIT compiler allows the language compiler, such as Scalac, to focus on other concerns. In this paper, we show how optimizations, such as inlining, polymorphic inlining, and partial escape analysis, are combined in Graal to produce collections code that is optimal with respect to manually written code, or close to optimal. We argue why some of these optimizations are more effectively done by a JIT compiler. We then identify specific use-cases that most current JIT compilers do not optimize well, warranting special treatment from the language compiler.},
	booktitle = {Proceedings of the 8th ACM SIGPLAN International Symposium on Scala},
	pages = {29–40},
	numpages = {12},
	keywords = {collections, JIT compilation, inlining, partial escape analysis, functional combinators, data-parallelism, scalar replacement, iterators, program optimization},
	location = {Vancouver, BC, Canada},
	series = {SCALA 2017}
}

@inproceedings{scala:calculus,
	title={A core calculus for Scala type checking},
	author={Cremet, Vincent and Garillot, Fran{\c{c}}ois and Lenglet, Sergue{\"\i} and Odersky, Martin},
	booktitle={International Symposium on Mathematical Foundations of Computer Science},
	pages={1--23},
	year={2006},
	organization={Springer}
}

@book{java:lang-spec,
	title={The Java language specification},
	author={Gosling, James and Joy, Bill and Steele, Guy and Bracha, Gilad},
	year={2000},
	publisher={Addison-Wesley Professional}
}

@article{java:escape-analysis,
	author = {Blanchet, Bruno},
	title = {Escape Analysis for JavaTM: Theory and Practice},
	year = {2003},
	issue_date = {November 2003},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {25},
	number = {6},
	issn = {0164-0925},
	url = {https://doi.org/10.1145/945885.945886},
	doi = {10.1145/945885.945886},
	abstract = {Escape analysis is a static analysis that determines whether the lifetime of data may exceed its static scope.This paper first presents the design and correctness proof of an escape analysis for JavaTM. This analysis is interprocedural, context sensitive, and as flow-sensitive as the static single assignment form. So, assignments to object fields are analyzed in a flow-insensitive manner. Since Java is an imperative language, the effect of assignments must be precisely determined. This goal is achieved thanks to our technique using two interdependent analyses, one forward, one backward. We introduce a new method to prove the correctness of this analysis, using aliases as an intermediate step. We use integers to represent the escaping parts of values, which leads to a fast and precise analysis.Our implementation [Blanchet 1999], which applies to the whole Java language, is then presented. Escape analysis is applied to stack allocation and synchronization elimination. In our benchmarks, we stack allocate 13% to 95% of data, eliminate more than 20% of synchronizations on most programs (94% and 99% on two examples) and get up to 43% runtime decrease (21% on average). Our detailed experimental study on large programs shows that the improvement comes more from the decrease of the garbage collection and allocation times than from improvements on data locality, contrary to what happened for ML. This comes from the difference in the garbage collectors.},
	journal = {ACM Trans. Program. Lang. Syst.},
	month = {nov},
	pages = {713–775},
	numpages = {63},
	keywords = {static analysis, optimization, synchronization elimination, Java, stack allocation}
}

@inproceedings{java:partial-escape-analysis,
	author = {Stadler, Lukas and W\"{u}rthinger, Thomas and M\"{o}ssenb\"{o}ck, Hanspeter},
	title = {Partial Escape Analysis and Scalar Replacement for Java},
	year = {2014},
	isbn = {9781450326704},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2544137.2544157},
	doi = {10.1145/2544137.2544157},
	abstract = {Escape Analysis allows a compiler to determine whether an object is accessible outside the allocating method or thread. This information is used to perform optimizations such as Scalar Replacement, Stack Allocation and Lock Elision, allowing modern dynamic compilers to remove some of the abstractions introduced by advanced programming models.The all-or-nothing approach taken by most Escape Analysis algorithms prevents all these optimizations as soon as there is one branch where the object escapes, no matter how unlikely this branch is at runtime.This paper presents a new, practical algorithm that performs control flow sensitive Partial Escape Analysis in a dynamic Java compiler. It allows Escape Analysis, Scalar Replacement and Lock Elision to be performed on individual branches. We implemented the algorithm on top of Graal, an open-source Java just-in-time compiler, and it performs well on a diverse set of benchmarks.In this paper, we evaluate the effect of Partial Escape Analysis on the DaCapo, ScalaDaCapo and SpecJBB2005 benchmarks, in terms of run-time, number and size of allocations and number of monitor operations. It performs particularly well in situations with additional levels of abstraction, such as code generated by the Scala compiler. It reduces the amount of allocated memory by up to 58.5%, and improves performance by up to 33%.},
	booktitle = {Proceedings of Annual IEEE/ACM International Symposium on Code Generation and Optimization},
	pages = {165–174},
	numpages = {10},
	keywords = {speculative optimization, java, escape analysis, virtual machine, just-in-time compilation, intermediate representation},
	location = {Orlando, FL, USA},
	series = {CGO '14}
}

@inproceedings{java:escape-analysis-optimizations,
	author = {Kotzmann, Thomas and Mossenbock, Hanspeter},
	title = {Run-Time Support for Optimizations Based on Escape Analysis},
	year = {2007},
	isbn = {0769527647},
	publisher = {IEEE Computer Society},
	address = {USA},
	url = {https://doi.org/10.1109/CGO.2007.34},
	doi = {10.1109/CGO.2007.34},
	abstract = {The JavaTM programming language does not allow the programmer to influence memory management. An object is usually allocated on the heap and deallocated by the garbage collector when it is not referenced any longer. Under certain conditions, the virtual machine can allocate objects on the stack or eliminate their allocation via scalar replacement. However, even if the dynamic compiler guarantees that the conditions are fulfilled, the optimizations require support by the run-time environment. We implemented a new escape analysis algorithm for Sun Microsystems' Java HotSpotTM VM. The results are used to replace objects with scalar variables, to allocate objects on the stack, and to remove synchronization. This paper deals with the representation of optimized objects in the debugging information and with reallocation and garbage collection support for a safe execution of optimized methods. Assignments to fields of parameters that can refer to both stack and heap objects are associated with an extended write barrier which skips card marking for stack objects. The traversal of objects during garbage collection uses a wrapper that abstracts from stack objects and presents their pointer fields as root pointers to the garbage collector. When a previously compiled and currently executing method must be continued in the interpreter because dynamic class loading invalidates the machine code, execution is suspended and compiler optimizations are undone. Scalar-replaced objects are reallocated on the heap lazily when control returns to the invalidated method, whereas stack-allocated objects must be reallocated immediately before program execution resumes. After reallocation, objects for which synchronization was removed are relocked.},
	booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
	pages = {49–60},
	numpages = {12},
	series = {CGO '07}
}

@book{java:generics,
	title={Java Generics and Collections: Speed Up the Java Development Process},
	author={Naftalin, Maurice and Wadler, Philip},
	year={2006},
	publisher={" O'Reilly Media, Inc."}
}

@misc{java:autoboxing,
	title = {Autoboxing and {Unboxing} ({The} {Java}™ {Tutorials} {\textgreater} {Learning} the {Java} {Language} {\textgreater} {Numbers} and {Strings})},
	url = {https://docs.oracle.com/javase/tutorial/java/data/autoboxing.html},
	urldate = {2022-07-25},
	file = {Autoboxing and Unboxing (The Java™ Tutorials > Learning the Java Language > Numbers and Strings):/home/jyou/snap/zotero-snap/common/Zotero/storage/ZWTRHV98/autoboxing.html:text/html},
}

@inproceedings{java:hotspot,
	title={The Java $\{$HotSpot™$\}$ Server Compiler},
	author={Paleczny, Michael and Vick, Christopher and Click, Cliff},
	booktitle={Java (TM) Virtual Machine Research and Technology Symposium (JVM 01)},
	year={2001}
}

@inproceedings{java:sablevm,
	title={Sable VM: A research framework for the efficient execution of java bytecode},
	author={Gagnon, Etienne M and Hendren, Laurie J},
	booktitle={Java Virtual Machine Research and Technology Symposium},
	pages={27--40},
	year={2001}
}

@misc{java:jikesrvm,
	title = {{IBM} {Research} {\textbar} {Technical} {Paper} {Search} {\textbar} {The} {Jikes} {RVM} {Project}: {Building} an {Open} {Source} {Research} {Community}({Search} {Reports})},
	copyright = {© Copyright IBM Corp. 2016},
	shorttitle = {{IBM} {Research} {\textbar} {Technical} {Paper} {Search} {\textbar} {The} {Jikes} {RVM} {Project}},
	url = {https://dominoweb.draco.res.ibm.com/0df2d86d1d71cd6e85256f790050b4c3.html},
	abstract = {The The Jikes RVM Project: Building an Open Source Research Community report web page.},
	language = {en},
	urldate = {2022-07-27},
	month = sep,
	year = {2016},
	file = {Snapshot:/home/jyou/snap/zotero-snap/common/Zotero/storage/J9RRFM6J/0df2d86d1d71cd6e85256f790050b4c3.html:text/html},
}

@inproceedings{java:graalvm,
address = {Indianapolis, Indiana, USA},
title = {One {VM} to {Rule} {Them} {All}},
isbn = {978-1-4503-2472-4},
url = {http://dl.acm.org/citation.cfm?doid=2509578.2509581},
doi = {10.1145/2509578.2509581},
abstract = {Building high-performance virtual machines is a complex and expensive undertaking; many popular languages still have low-performance implementations. We describe a new approach to virtual machine (VM) construction that amortizes much of the effort in initial construction by allowing new languages to be implemented with modest additional effort. The approach relies on abstract syntax tree (AST) interpretation where a node can rewrite itself to a more specialized or more general node, together with an optimizing compiler that exploits the structure of the interpreter. The compiler uses speculative assumptions and deoptimization in order to produce efﬁcient machine code. Our initial experience suggests that high performance is attainable while preserving a modular and layered architecture, and that new highperformance language implementations can be obtained by writing little more than a stylized interpreter.},
language = {en},
urldate = {2020-10-20},
booktitle = {Proceedings of the 2013 {ACM} {International} {Symposium} on {New} ideas, {New} {Paradigms}, and {Reflections} on {Programming} \& {Software} - {Onward}! '13},
publisher = {ACM Press},
author = {Würthinger, Thomas and Wimmer, Christian and Wöß, Andreas and Stadler, Lukas and Duboscq, Gilles and Humer, Christian and Richards, Gregor and Simon, Doug and Wolczko, Mario},
year = {2013},
pages = {187--204},
file = {Würthinger et al. - 2013 - One VM to Rule Them All.pdf:/home/jyou/snap/zotero-snap/common/Zotero/storage/7AS8H4Y6/Würthinger et al. - 2013 - One VM to Rule Them All.pdf:application/pdf},
}


@inproceedings{graalvm:ir,
	title = {Graal {IR}: {An} {Extensible} {Declarative} {Intermediate} {Representation}},
	shorttitle = {Graal {IR}},
	abstract = {We present an intermediate representation (IR) for a Java just in time (JIT) compiler written in Java. It is a graph-based IR that models both control-flow and data-flow dependencies between nodes. We show the framework in which we developed our IR. Much care has been taken to allow the programmer to focus on compiler optimization rather than IR bookkeeping. Edges between nodes are declared concisely using Java annotations, and common properties and functions on nodes are communicated to the framework by implementing interfaces. Building upon these declarations, the graph framework automatically implements a set of useful primitives that the programmer can use to implement optimizations .},
	author = {Duboscq, Gilles and Stadler, Lukas and Wuerthinger, Thomas and Simon, Doug and Wimmer, Christian and Mössenböck, Hanspeter},
	month = feb,
	year = {2013},
	file = {Full Text PDF:/home/jyou/snap/zotero-snap/common/Zotero/storage/WLX5AHLQ/Duboscq et al. - 2013 - Graal IR An Extensible Declarative Intermediate R.pdf:application/pdf},
}


@inproceedings{graalvm:speculative-ir,
	address = {Indianapolis, Indiana, USA},
	title = {An {Intermediate} {Representation} for {Speculative} {Optimizations} in a {Dynamic} {Compiler}},
	isbn = {978-1-4503-2601-8},
	url = {http://dl.acm.org/citation.cfm?doid=2542142.2542143},
	doi = {10.1145/2542142.2542143},
	abstract = {We present a compiler intermediate representation (IR) that allows dynamic speculative optimizations for high-level languages. The IR is graph-based and contains nodes ﬁxed to control ﬂow as well as ﬂoating nodes. Side-effecting nodes include a framestate that maps values back to the original program. Guard nodes dynamically check assumptions and, on failure, deoptimize to the interpreter that continues execution. Guards implicitly use the framestate and program position of the last side-effecting node. Therefore, they can be represented as freely ﬂoating nodes in the IR. Exception edges are modeled as explicit control ﬂow and are subject to full optimization. We use proﬁling and deoptimization to speculatively reduce the number of such edges. The IR is the core of a just-in-time compiler that is integrated with the Java HotSpot VM. We evaluate the design decisions of the IR using major Java benchmark suites.},
	language = {en},
	urldate = {2020-10-20},
	booktitle = {Proceedings of the 7th {ACM} workshop on {Virtual} {Machines} and {Intermediate} {Languages} - {VMIL} '13},
	publisher = {ACM Press},
	author = {Duboscq, Gilles and Würthinger, Thomas and Stadler, Lukas and Wimmer, Christian and Simon, Doug and Mössenböck, Hanspeter},
	year = {2013},
	pages = {1--10},
	file = {Duboscq et al. - 2013 - An Intermediate Representation for Speculative Opt.pdf:/home/jyou/snap/zotero-snap/common/Zotero/storage/MRTS3L3V/Duboscq et al. - 2013 - An Intermediate Representation for Speculative Opt.pdf:application/pdf},
}



@article{simula:classes,
	title={SIMULA: an ALGOL-based simulation language},
	author={Dahl, Ole-Johan and Nygaard, Kristen},
	journal={Communications of the ACM},
	volume={9},
	number={9},
	pages={671--678},
	year={1966},
	publisher={ACM New York, NY, USA}
}

@book{smalltalk:design,
	title={Smalltalk-80: the language and its implementation},
	author={Goldberg, Adele and Robson, David},
	year={1983},
	publisher={Addison-Wesley Longman Publishing Co., Inc.}
}


@inproceedings{smalltalk:inline-caches,
	author = {Deutsch, L. Peter and Schiffman, Allan M.},
	title = {Efficient Implementation of the Smalltalk-80 System},
	year = {1984},
	isbn = {0897911253},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/800017.800542},
	doi = {10.1145/800017.800542},
	abstract = {The Smalltalk-80* programming language includes dynamic storage allocation, full upward funargs, and universally polymorphic procedures; the Smalltalk-80 programming system features interactive execution with incremental compilation, and implementation portability. These features of modern programming systems are among the most difficult to implement efficiently, even individually. A new implementation of the Smalltalk-80 system, hosted on a small microprocessor-based computer, achieves high performance while retaining complete (object code) compatibility with existing implementations. This paper discusses the most significant optimization techniques developed over the course of the project, many of which are applicable to other languages. The key idea is to represent certain runtime state (both code and data) in more than one form, and to convert between forms when needed.},
	booktitle = {Proceedings of the 11th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
	pages = {297–302},
	numpages = {6},
	location = {Salt Lake City, Utah, USA},
	series = {POPL '84}
}

@inproceedings{self:polymorphic-inline-caches,
	author="H{\"o}lzle, Urs
	and Chambers, Craig
	and Ungar, David",
	editor="America, Pierre",
	title="Optimizing dynamically-typed object-oriented languages with polymorphic inline caches",
	booktitle="ECOOP'91 European Conference on Object-Oriented Programming",
	year="1991",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="21--38",
	abstract="Polymorphic inline caches (PICs) provide a new way to reduce the overhead of polymorphic message sends by extending inline caches to include more than one cached lookup result per call site. For a set of typical object-oriented SELF programs, PICs achieve a median speedup of 11{\%}.",
	isbn="978-3-540-47537-8"
}

@article{10.1145/143103.143114,
	author = {H\"{o}lzle, Urs and Chambers, Craig and Ungar, David},
	title = {Debugging Optimized Code with Dynamic Deoptimization},
	year = {1992},
	issue_date = {July 1992},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {27},
	number = {7},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/143103.143114},
	doi = {10.1145/143103.143114},
	abstract = {SELF's debugging system provides complete source-level debugging (expected behavior) with globally optimized code. It shields the debugger from optimizations performed by the compiler by dynamically deoptimizing code on demand. Deoptimization only affects the procedure activations that are actively being debugged; all other code runs at full speed. Deoptimization requires the compiler to supply debugging information at discrete interrupt points; the compiler can still perform extensive optimizations between interrupt points without affecting debuggability. At the same time, the inability to interrupt between interrupt points is invisible to the user. Our debugging system also handles programming changes during debugging. Again, the system provides expected behavior: it is possible to change a running program and immediately observe the effects of the change. Dynamic deoptimization transforms old compiled code (which may contain inlined copies of the old version of the changed procedure) into new versions reflecting the current source-level state. To the best of our knowledge, SELF is the first practical system providing full expected behavior with globally optimized code.},
	journal = {SIGPLAN Not.},
	month = {jul},
	pages = {32–43},
	numpages = {12}
}

@inproceedings{self:deoptimization,
	author = {H\"{o}lzle, Urs and Chambers, Craig and Ungar, David},
	title = {Debugging Optimized Code with Dynamic Deoptimization},
	year = {1992},
	isbn = {0897914759},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/143095.143114},
	doi = {10.1145/143095.143114},
	abstract = {SELF's debugging system provides complete source-level debugging (expected behavior) with globally optimized code. It shields the debugger from optimizations performed by the compiler by dynamically deoptimizing code on demand. Deoptimization only affects the procedure activations that are actively being debugged; all other code runs at full speed. Deoptimization requires the compiler to supply debugging information at discrete interrupt points; the compiler can still perform extensive optimizations between interrupt points without affecting debuggability. At the same time, the inability to interrupt between interrupt points is invisible to the user. Our debugging system also handles programming changes during debugging. Again, the system provides expected behavior: it is possible to change a running program and immediately observe the effects of the change. Dynamic deoptimization transforms old compiled code (which may contain inlined copies of the old version of the changed procedure) into new versions reflecting the current source-level state. To the best of our knowledge, SELF is the first practical system providing full expected behavior with globally optimized code.},
	booktitle = {Proceedings of the ACM SIGPLAN 1992 Conference on Programming Language Design and Implementation},
	pages = {32–43},
	numpages = {12},
	location = {San Francisco, California, USA},
	series = {PLDI '92}
}

@inproceedings{ml:parametric-polymorphism,
	title = "A Logic for Computable Functions with Reflexive and Polymorphic Types",
	keywords = "rcb-bibfile",
	author = "R. Milner and L. Morris and M. Newey",
	year = "1975",
	language = "English",
	pages = "371--394",
	booktitle = "Proceedings of the Conference on Proving and Improving Programs",
	publisher = "IRIA-Laboria",
}

@inproceedings{ml:type-inference,
	title={Principal type-schemes for functional programs},
	author={Damas, Luis and Milner, Robin},
	booktitle={Proceedings of the 9th ACM SIGPLAN-SIGACT symposium on Principles of programming languages},
	pages={207--212},
	year={1982}
}


@article{scheme:closures,
	title={Scheme: A interpreter for extended lambda calculus},
	author={Sussman, Gerald Jay and Steele, Guy L},
	journal={Higher-Order and Symbolic Computation},
	volume={11},
	number={4},
	pages={405--439},
	year={1998},
	publisher={Springer}
}


@article{clr:spec,
	title={Technical overview of the common language runtime},
	author={Meijer, Erik and Gough, John},
	journal={language},
	volume={29},
	number={7},
	year={2001},
	publisher={Citeseer}
}

@inproceedings{llvm,
	title={LLVM: A compilation framework for lifelong program analysis \& transformation},
	author={Lattner, Chris and Adve, Vikram},
	booktitle={International Symposium on Code Generation and Optimization, 2004. CGO 2004.},
	pages={75--86},
	year={2004},
	organization={IEEE}
}


@article{ssa,
	author = {Ron Cytron and Jeanne Ferrante and Barry K. Rosen and Mark
	N. Wegman and F. Kenneth Zadeck},
	title = {Efficiently Computing Static Single Assignment Form and the
	Control Dependence Graph},
	journal = {ACM Transactions on Programming Languages and Systems},
	volume = {13},
	number = {4},
	month = {Oct},
	publisher = {ACM Press},
	pages = {451--490},
	year = {1991},
	abstract = {The most important paper in the field. Comprehensive yet
	readable. Note there was an earlier version at POPL 89.},
	url = {http://doi.acm.org/10.1145/115372.115320}
}

@inproceedings{hsail,
	title={HSAIL: Portable compiler IR for HSA.},
	author={Sander, Ben and FELLOW, AMD SENIOR},
	booktitle={Hot Chips Symposium},
	volume={2013},
	pages={1--32},
	year={2013}
}

@article{futamura:partial-eval,
	title={Partial evaluation of computation process--an approach to a compiler-compiler},
	author={Futamura, Yoshihiko},
	journal={Higher-Order and Symbolic Computation},
	volume={12},
	number={4},
	pages={381--391},
	year={1999},
	publisher={Springer}
}

@article{tofte:region-memory,
	title = {Region-Based Memory Management},
	journal = {Information and Computation},
	volume = {132},
	number = {2},
	pages = {109-176},
	year = {1997},
	issn = {0890-5401},
	doi = {https://doi.org/10.1006/inco.1996.2613},
	url = {https://www.sciencedirect.com/science/article/pii/S0890540196926139},
	author = {Mads Tofte and Jean-Pierre Talpin},
	abstract = {This paper describes a memory management discipline for programs that perform dynamic memory allocation and de-allocation. At runtime, all values are put intoregions. The store consists of a stack of regions. All points of region allocation and de-allocation are inferred automatically, using a type and effect based program analysis. The scheme does not assume the presence of a garbage collector. The scheme was first presented in 1994 (M. Tofte and J.-P. Talpin,in“Proceedings of the 21st ACM SIGPLAN–SIGACT Symposium on Principles of Programming Languages,” pp. 188–201); subsequently, it has been tested in The ML Kit with Regions, a region-based, garbage-collection free implementation of the Standard ML Core language, which includes recursive datatypes, higher-order functions and updatable references L. Birkedal, M. Tofte, and M. Vejlstrup, (1996),in“Proceedings of the 23 rd ACM SIGPLAN–SIGACT Symposium on Principles of Programming Languages,” pp. 171–183. This paper defines a region-based dynamic semantics for a skeletal programming language extracted from Standard ML. We present the inference system which specifies where regions can be allocated and de-allocated and a detailed proof that the system is sound with respect to a standard semantics. We conclude by giving some advice on how to write programs that run well on a stack of regions, based on practical experience with the ML Kit.}
}

@inproceedings{aiken:region-memory-analysis,
	author = {Aiken, Alexander and F\"{a}hndrich, Manuel and Levien, Raph},
	title = {Better Static Memory Management: Improving Region-Based Analysis of Higher-Order Languages},
	year = {1995},
	isbn = {0897916972},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/207110.207137},
	doi = {10.1145/207110.207137},
	abstract = {Static memory management replaces runtime garbage collection with compile-time annotations that make all memory allocation and deallocation explicit in a program. We improve upon the Tofte/Talpin region-based scheme for compile-time memory management[TT94]. In the Tofte/Talpin approach, all values, including closures, are stored in regions. Region lifetimes coincide with lexical scope, thus forming a runtime stack of regions and eliminating the need for garbage collection. We relax the requirement that region lifetimes be lexical. Rather, regions are allocated late and deallocated as early as possible by explicit memory operations. The placement of allocation and deallocation annotations is determined by solving a system of constraints that expresses all possible annotations. Experiments show that our approach reduces memory requirements significantly, in some cases asymptotically.},
	booktitle = {Proceedings of the ACM SIGPLAN 1995 Conference on Programming Language Design and Implementation},
	pages = {174–185},
	numpages = {12},
	location = {La Jolla, California, USA},
	series = {PLDI '95}
}

@inproceedings{birkedal:region-memory-inference,
	author = {Birkedal, Lars and Tofte, Mads and Vejlstrup, Magnus},
	title = {From Region Inference to von Neumann Machines via Region Representation Inference},
	year = {1996},
	isbn = {0897917693},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/237721.237771},
	doi = {10.1145/237721.237771},
	abstract = {Region Inference is a technique for implementing programming languages that are based on typed call-by-value lambda calculus, such as Standard ML. The mathematical runtime model of region inference uses a stack of regions, each of which can contain an unbounded number of values. This paper is concerned with mapping the mathematical model onto real machines. This is done by composing region inference with Region Representation Inference, which gradually refines region information till it is directly implementable on conventional von Neumann machines. The performance of a new region-based ML compiler is compared to the performance of Standard ML of New Jersey, a state-of-the-art ML compiler.},
	booktitle = {Proceedings of the 23rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
	pages = {171–183},
	numpages = {13},
	location = {St. Petersburg Beach, Florida, USA},
	series = {POPL '96}
}

@article{strachey:fundamental-concepts,
	title={Fundamental concepts in programming languages},
	author={Strachey, Christopher},
	journal={Higher-order and symbolic computation},
	volume={13},
	number={1},
	pages={11--49},
	year={2000},
	publisher={Springer}
}

@inproceedings{go4:design-patterns,
	title={Design patterns: Abstraction and reuse of object-oriented design},
	author={Gamma, Erich and Helm, Richard and Johnson, Ralph and Vlissides, John},
	booktitle={European Conference on Object-Oriented Programming},
	pages={406--431},
	year={1993},
	organization={Springer}
}


@article{click:sea-of-nodes,
	title = {Combining analyses, combining optimizations},
	volume = {17},
	issn = {0164-0925, 1558-4593},
	url = {https://dl.acm.org/doi/10.1145/201059.201061},
	doi = {10.1145/201059.201061},
	abstract = {This thesis presents a framework for describing optimizations. It shows how to combine two such frameworks and how to reason about the properties of the resulting framework. The structure of the framework provides insight into when a combination yields better results. Also presented is a simple iterative algorithm for solving these frameworks. A framework is shown that combines Constant Propagation, Unreachable Code Elimination, Global Congruence Finding and Global Value Numbering. For these optimizations, the iterative algorithm runs in O(n2) time.},
	language = {en},
	number = {2},
	urldate = {2022-02-15},
	journal = {ACM Transactions on Programming Languages and Systems},
	author = {Click, Cliff and Cooper, Keith D.},
	month = mar,
	year = {1995},
	pages = {181--196},
	file = {Click and Cooper - 1995 - Combining analyses, combining optimizations.pdf:/home/jyou/snap/zotero-snap/common/Zotero/storage/R7XKVHAK/Click and Cooper - 1995 - Combining analyses, combining optimizations.pdf:application/pdf},
}

@inproceedings{allen:ctrl-flow-analysis,
	author = {Allen, Frances E.},
	title = {Control Flow Analysis},
	year = {1970},
	isbn = {9781450373869},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/800028.808479},
	doi = {10.1145/800028.808479},
	abstract = {Any static, global analysis of the expression and data relationships in a program requires a knowledge of the control flow of the program. Since one of the primary reasons for doing such a global analysis in a compiler is to produce optimized programs, control flow analysis has been embedded in many compilers and has been described in several papers. An early paper by Prosser [5] described the use of Boolean matrices (or, more particularly, connectivity matrices) in flow analysis. The use of “dominance” relationships in flow analysis was first introduced by Prosser and much expanded by Lowry and Medlock [6]. References [6,8,9] describe compilers which use various forms of control flow analysis for optimization. Some recent developments in the area are reported in [4] and in [7].The underlying motivation in all the different types of control flow analysis is the need to codify the flow relationships in the program. The codification may be in connectivity matrices, in predecessor-successor tables, in dominance lists, etc. Whatever the form, the purpose is to facilitate determining what the flow relationships are; in other words to facilitate answering such questions as: is this an inner loop?, if an expression is removed from the loop where can it be correctly and profitably placed?, which variable definitions can affect this use?In this paper the basic control flow relationships are expressed in a directed graph. Various graph constructs are then found and shown to codify interesting global relationships.},
	booktitle = {Proceedings of a Symposium on Compiler Optimization},
	pages = {1–19},
	numpages = {19},
	location = {Urbana-Champaign, Illinois}
}

@article{allen:data-flow-analysis,
	author = {Allen, F. E. and Cocke, J.},
	title = {A Program Data Flow Analysis Procedure},
	year = {1976},
	issue_date = {March 1976},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {19},
	number = {3},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/360018.360025},
	doi = {10.1145/360018.360025},
	abstract = {The global data relationships in a program can be exposed and codified by the static analysis methods described in this paper. A procedure is given which determines all the definitions which can possibly “reach” each node of the control flow graph of the program and all the definitions that are “live” on each edge of the graph. The procedure uses an “interval” ordered edge listing data structure and handles reducible and irreducible graphs indistinguishably.},
	journal = {Commun. ACM},
	month = {mar},
	pages = {137},
	numpages = {11},
	keywords = {program optimization, compilers, data flow analysis, flow graphs, algorithms}
}

@inproceedings{johnson:use-def-chains,
	title={Dependence-based program analysis},
	author={Richard Johnson and Keshav Pingali},
	booktitle={PLDI '93},
	year={1993}
}

@article{10.1145/97946.97982,
	author = {Bracha, Gilad and Cook, William},
	title = {Mixin-Based Inheritance},
	year = {1990},
	issue_date = {Oct. 1990},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {25},
	number = {10},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/97946.97982},
	doi = {10.1145/97946.97982},
	abstract = {The diverse inheritance mechanisms provided by Smalltalk, Beta, and CLOS are interpreted as different uses of a single underlying construct. Smalltalk and Beta differ primarily in the direction of class hierarchy growth. These inheritance mechanisms are subsumed in a new inheritance model based on composition of mixins, or abstract subclasses. This form of inheritance can also encode a CLOS multiple-inheritance hierarchy, although changes to the encoded hierarchy that would violate encapsulation are difficult. Practical application of mixin-based inheritance is illustrated in a sketch of an extension to Modula-3.},
	journal = {SIGPLAN Not.},
	month = {sep},
	pages = {303–311},
	numpages = {9}
}

@inproceedings{gilad:mixins,
	author = {Bracha, Gilad and Cook, William},
	title = {Mixin-Based Inheritance},
	year = {1990},
	isbn = {0897914112},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/97945.97982},
	doi = {10.1145/97945.97982},
	abstract = {The diverse inheritance mechanisms provided by Smalltalk, Beta, and CLOS are interpreted as different uses of a single underlying construct. Smalltalk and Beta differ primarily in the direction of class hierarchy growth. These inheritance mechanisms are subsumed in a new inheritance model based on composition of mixins, or abstract subclasses. This form of inheritance can also encode a CLOS multiple-inheritance hierarchy, although changes to the encoded hierarchy that would violate encapsulation are difficult. Practical application of mixin-based inheritance is illustrated in a sketch of an extension to Modula-3.},
	booktitle = {Proceedings of the European Conference on Object-Oriented Programming on Object-Oriented Programming Systems, Languages, and Applications},
	pages = {303–311},
	numpages = {9},
	location = {Ottawa, Canada},
	series = {OOPSLA/ECOOP '90}
}

@article{10.1145/773473.178260,
	author = {Srivastava, Amitabh and Eustace, Alan},
	title = {ATOM: A System for Building Customized Program Analysis Tools},
	year = {1994},
	issue_date = {June 1994},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {29},
	number = {6},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/773473.178260},
	doi = {10.1145/773473.178260},
	abstract = {ATOM (Analysis Tools with OM) is a single framework for building a wide range of customized program analysis tools. It provides the common infrastructure present in all code-instrumenting tools; this is the difficult and time-consuming part. The user simply defines the tool-specific details in instrumentation and analysis routines. Building a basic block counting tool like Pixie with ATOM requires only a page of code.ATOM, using OM link-time technology, organizes the final executable such that the application program and user's analysis routines run in the same address space. Information is directly passed from the application program to the analysis routines through simple procedure calls instead of inter-process communication or files on disk. ATOM takes care that analysis routines do not interfere with the program's execution, and precise information about the program is presented to the analysis routines at all times. ATOM uses no simulation or interpretation.ATOM has been implemented on the Alpha AXP under OSF/1. It is efficient and has been used to build a diverse set of tools for basic block counting, profiling, dynamic memory recording, instruction and data cache simulation, pipeline simulation, evaluating branch prediction, and instruction scheduling.},
	journal = {SIGPLAN Not.},
	month = {jun},
	pages = {196–205},
	numpages = {10}
}



@inproceedings{profiling:atom,
	author = {Srivastava, Amitabh and Eustace, Alan},
	title = {ATOM: A System for Building Customized Program Analysis Tools},
	year = {1994},
	isbn = {089791662X},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/178243.178260},
	doi = {10.1145/178243.178260},
	abstract = {ATOM (Analysis Tools with OM) is a single framework for building a wide range of customized program analysis tools. It provides the common infrastructure present in all code-instrumenting tools; this is the difficult and time-consuming part. The user simply defines the tool-specific details in instrumentation and analysis routines. Building a basic block counting tool like Pixie with ATOM requires only a page of code.ATOM, using OM link-time technology, organizes the final executable such that the application program and user's analysis routines run in the same address space. Information is directly passed from the application program to the analysis routines through simple procedure calls instead of inter-process communication or files on disk. ATOM takes care that analysis routines do not interfere with the program's execution, and precise information about the program is presented to the analysis routines at all times. ATOM uses no simulation or interpretation.ATOM has been implemented on the Alpha AXP under OSF/1. It is efficient and has been used to build a diverse set of tools for basic block counting, profiling, dynamic memory recording, instruction and data cache simulation, pipeline simulation, evaluating branch prediction, and instruction scheduling.},
	booktitle = {Proceedings of the ACM SIGPLAN 1994 Conference on Programming Language Design and Implementation},
	pages = {196–205},
	numpages = {10},
	location = {Orlando, Florida, USA},
	series = {PLDI '94}
}






