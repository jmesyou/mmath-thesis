\chapter{Future Work}

TastyTruffle is intended to be framework for dynamic whole-program approaches to optimizing Scala.
In this section, we discuss some possible extensions to the interpreter that further takes advantage of Truffle mechanisms.
A substantial penalty of heterogeneous translations of polymorphic programs is \textit{code explosion}.
For large polymorphic programs, the penalty of heterogeneous translation is twofold; 
The first is the cost of increased memory usage.
Having many specialized data representations incurs extra storage, unless of course, these data representations are regenerated every time a specialization is needed.
The second is the hidden computational overhead of specialization. 
Like other computational overheads of managed runtimes such as garbage collection, time spent generating specialized variants of polymorphic classes or methods means time not spent executing the program.
We propose several methods to augment \textit{when} a specialization is created.

Many prior approaches to specialization have already attempted to minimize the number of specializations to mitigate performance degradation for complex polymorphic definitions, where is there often a \(O(t^n)\) space complexity worse case\footnote{$t$ is the number of value types combined with the reference type, $n$ is the number of type parameters in a generic definition.}, and very large programs.
These approaches balance the tradeoff between performance and code size to optimistically generate \textit{only} the specializations required to eliminate performance bottlenecks.
Because of the work done in \cite{scala:specialization}, many existing Scala programs are already user-annotated with a specialization directive.
Similarly, our approach could be extended to include the semantics of this annotation, generating specializations with user-guide information only where needed.
The translation of non-annotated definitions will simply use a shared type-erased data representation.
However, mixing annotated and non-annotated programs will presents missed optimization opportunities in the non-annotated portions of the program.

Truffle offers many existing mechanisms for profiling values and types.
Some of this profiling instrumentation is automatically done by Truffle, such as the profiling of argument types for node rewrites.
While other instrumentation,  such as condition profiles, are added by the guest language implementer.
These profiles augment partial evaluation and enable speculative assumptions to augment optimizations such as conditional elimination.
We propose instrumenting specialization sites to profile type arguments.
Type argument profiles could then be used to decide the specific instantiation to specialize.
A \textit{profile-guided} approach to specialization could limit specializations to only the most frequently used instantiations.

Often a polymorphic instantiation is not sufficiently frequent to warrant specialization, the default homogeneous data representation will be shared among unspecialized instantiation.
A type-erased homogeneous data representation may still be tagged with the underlying applied type.
We can further augment type-erased polymorphic fields and frame slots to profile reads from and writes to their respective storage locations.
These two pieces of dynamic information can be combined to allow the specialization of certain instantiations that are frequently manipulated, but not frequently created.

With inspiration from the work done by Ureche et al. in \cite{scala:miniboxing}, we can apply the same optimization to sharing data layouts between certain specializations.
Because the additional operations that adapt shared specializations to their original type contexts are simple neglible in terms of performance \cite{scala:miniboxing}, these operations make sense to intrinsify as Truffle nodes that will be further optimized by JIT compilation.